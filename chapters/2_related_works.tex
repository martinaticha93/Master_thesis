\chapter{Related Work}

In this chapter, we describe the major contributions to the person re-ID problem from the early years when person re-ID was a part of the person tracking problem, until today when person re-ID itself gains significant attention among other computer vision problems. The main focus of this thesis is the long-term person re-identification. However, the vast majority of the person re-ID approaches and benchmarks so far focus on the short-term person re-ID. Since both problems are closely related, we provide an overview of methods to either of these two. At the end of this chapter, we shortly describe the current long-term person re-ID approaches.

According to Zheng et al.\cite{Zheng_past_pres_future}, the first work, where the term "person re-identification" was used, was published in 2005 by Wojciech Zajdelet et al. \cite{first_publications}. In this work, the authors describe a system enabling a mobile robot, equipped with a color vision system, to track people and to keep track of them when they leave the field of view. Later on, the re-identification task separated from person tracking problem, and the focus was put on the person retrieval in multi-camera vision systems with the goal to find identical persons in footage from different cameras.


\section{Image-based person re-ID}
The first approaches to person re-ID focused on image matching rather than matching of the whole videos.
There are two sub-tasks the person re-ID systems need to solve: 
 
\begin{enumerate}[label=(\alph*)]
\item image description  - defines the feature vector that is extracted from the raw image pixels
\item similarity metric - defined to measure the similarity between two images
\end{enumerate}

Based on how the re-ID systems handle these two sub-tasks, we will categorize them same as in \cite{Zheng_past_pres_future} into hand-crafted systems and deep-learning systems.
\subsubsection{Hand-crafted systems}

The crucial step in hand-crafted re-ID systems is the choice of features extracted from the raw input data. The data needs to be examined, and the algorithms for feature extraction are then designed based on the properties of the data. Hence, the resulting re-ID systems are often tuned for the input data and do not generalize well when used on a different data set.

Most commonly used features in the pedestrian description are low-level features such as color, texture, or gradient. For color representation, Prosser et al. \cite{re_id_by_support_vector_ranking} and many others use eight color channels - RGB, HS, and YCbCr. In the same work, 21 texture filters (Gabor \cite{gabor_filters} and Schmid \cite{schmid_filters}) were applied for the luminance channel. Texture features, on the other hand, can be represented as Local Binary Patterns (LBP) \cite{pcca}. Low-level features can further be encoded into descriptors such as color histograms, fisher vectors \cite{fisher_vectors} or Histogram of Oriented Gradients \cite{HOG_human_detection}. The spacial structure of the image can be analyzed by dividing the image into a grid or stripes and the features extracted from these separated regions \cite{pcca}.

The second core element that needs to be chosen in hand-crafted systems is the distance metric that measures the similarity between two samples. The core idea of the similarity measurement is to keep the feature vector of the same identity close while maintaining a distance between different identities. According to \cite{Zheng_past_pres_future}, the most commonly used distance metric is based on the Mahalanobis distance function, which is a generalization of the Euclidean distance metric that uses linear scaling and rotations of the feature spaces. 
\subsubsection{Deep-learning systems}

In recent years, end-to-end convolutional neural networks (CNN) have been successfully applied in many areas of computer vision \cite{DL_for_vison}. The main advantage of CNNs is their ability to automatically learn the feature representation without any explicit specification of the feature extraction technique. Currently, most of the state-of-the-art re-ID methods are based on deep-learning systems as they have significantly outperformed the previously mentioned hand-crafted systems. 

The first works in re-ID that used deep learning techniques were \cite{deep_for_re_id} and \cite{deep_filters_for_re_id}. Since then, two main approaches to person re-ID in deep learning emerged: classification  models and verification models (see \cite{Zheng_past_pres_future} for more details). 

In classification models, the training data consist of labeled images categorized into classes. The input to the neural network is then a single image of an identity which needs to be classified by the model. 

Verification models, on the other hand, take a set of images as their inputs and estimate the similarity between them. One example of the verification model is the Siamese neural networks \cite{siamese_network}. This network consists of two identical sub-networks whose outputs are connected by a joining neuron. The training data consists of pairs of samples which are labeled as either the same or different class. The network is then trained to recognize the data from the same class.

\section{Video-based person re-ID}
In recent years, due to the increasing data richness, video-based person re-ID is gaining more and more attention in research. In this case, the re-ID algorithm consumes the whole sequence of images as its input, and so the algorithm can collect more information about the appearance of each individual. Moreover, when keeping the input as a sequence, spatial-temporal features can be extracted for each identity. This way, the model gets additional information about movements. Similarly, as in image-based person re-ID systems, there are two groups of video-based person re-ID systems:
\subsubsection{Hand-crafted systems}
In 2010, Bazzani et al.\cite{multi_shot_person_reid} proposed a work where a set of images of the same individuality is condensed into a highly informative signature, which is then matched with the signature of another set of images. It shows that using multiple frames per person instead of a single image significantly improves the results of the re-ID. Such methods are usually denoted as "multi-shot person re-ID". The disadvantage of the multi-shot strategy is that it does not incorporate any temporal cues in the model. In 2014, Wang et al. \cite{video_ranking_re-id} proposed a method that automatically selects the most discriminative video segment from a noisy image sequence. Space-time features are then extracted from this segment. In \cite{fisher_composition_for_video_reid}, the video sequences are first decomposed into sequences of individual body units, from which Fisher vectors are extracted and combined into a descriptor of the whole video sequence. Gao at al. \cite{temporally_aligned_pooling} proposes a method that uses the periodicity property of pedestrians of the "best" walking cycle from noisy motion information. To describe the video data in the selected walking cycle, the cycle is first divided into several segments. Each segment is then described by temporally aligned pooling.  
\subsubsection{Deep-learning systems}
In video-based re-ID deep-learning systems, in order to utilize the spatial-temporal features, the neural network is usually constructed so that it takes the whole sequence of images as its input. There are two prevailing approaches to achieve this. Pooling based methods aggregate appearance features (e.g., CNN, color, LBP) extracted from individual images into a vector representing the whole video sequence \cite{MARS}. The main disadvantage of this method is that it cannot model well the temporal changes in human pose. The second popular method for video-based re-ID utilizes the schema of recurrent neural networks (RNN), which are special types of a neural network that can aggregate both image-level features and human dynamics information. In \cite{recurrent_feature_aggregation_lstm}, a special type of RNN called Long Short-Term Memory (LSTM) is fed with low-level features extracted from individual frames of input video sequences. The outputs of several such LSTMs are then connected to a softmax layer. N. McLaughlin at al.  \cite{pooling_with_RNN} uses a convolutional neural network that incorporates a recurrent final layer which enables the features extracted from individual frames to be enriched by features from previously seen frames. The outputs from this neural network are then combined using temporal pooling, which gives an appearance feature of the whole sequence  \cite{pooling_with_RNN}. 
\section{Long-term person re-ID}
All the above-mentioned methods are based on appearance matching of the observe identities, which limits their application to a short-term person re-ID. To build a model that is able to re-identify a person after a long time when visual features like clothing are no more reliable, one needs to utilize personal characteristics that do not change by the time and that are unique for each identity. To my knowledge, the only contribution to long-term person re-ID so far is in 2018 by Zhang et al. \cite{long-term_re-id_true_motion}. In this work, partially inspired by the success of dense trajectory on action recognition \cite{action_recognition}, the authors present a model based on a hypothesis that people keep constant motion patterns under non-distraction walking condition. 
